{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3da48fd",
   "metadata": {},
   "source": [
    "# Code Conversion Pipeline - Interactive Testing\n",
    "\n",
    "This notebook provides interactive testing for the entire code conversion pipeline:\n",
    "- **Input Layer**: Repository scanning, file classification, and preprocessing\n",
    "- **Analysis Layer**: Code segmentation, dependency analysis, and context extraction\n",
    "- **Validation Layer**: LLM validation, semantic validation, and performance analysis\n",
    "\n",
    "You can test individual files or entire repositories interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a55380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Add current directory to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Pipeline imports\n",
    "from app.input.repo_scanner import RepositoryScanner\n",
    "from app.input.file_classifier import FileClassifier\n",
    "from app.input.preprocessing import Preprocessor\n",
    "from app.analysis.context_extractor import extract_context\n",
    "from app.analysis.dependency_analyzer import DependencyAnalyzer\n",
    "from app.analysis.segmentation_engine import SegmentationEngine\n",
    "from app.validation.validation_controller import ValidationController\n",
    "from app.validation.llm_validator import LLMValidator\n",
    "from app.validation.semantic_validator import SemanticValidator\n",
    "from app.validation.performance_analyzer import PerformanceAnalyzer\n",
    "from app.config import config\n",
    "\n",
    "print(\" All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e92ed",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0698c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_github_repo(git_url: str) -> str:\n",
    "    \"\"\"Clone a GitHub repository into a temporary directory and return the path.\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    print(f\" Cloning {git_url} into {temp_dir}...\")\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", git_url, temp_dir], check=True, stdout=subprocess.DEVNULL)\n",
    "        print(f\" Successfully cloned repository\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\" Failed to clone repo: {e}\")\n",
    "        shutil.rmtree(temp_dir)\n",
    "        raise e\n",
    "    return temp_dir\n",
    "\n",
    "def print_segment_details(segment: Dict[str, Any], index: int, file_path: str):\n",
    "    \"\"\"Print detailed information about a segment\"\"\"\n",
    "    print(f\"\\n   Segment {index + 1}:\")\n",
    "    print(f\"    Name: {segment.get('name', '<anonymous>')}\")\n",
    "    print(f\"    Type: {segment.get('node_type', 'unknown')}\")\n",
    "    \n",
    "    # Show line numbers if available\n",
    "    if 'start_line' in segment and 'end_line' in segment:\n",
    "        print(f\"    Lines: {segment['start_line'] + 1}-{segment['end_line'] + 1}\")\n",
    "    elif 'start_byte' in segment and 'end_byte' in segment:\n",
    "        print(f\"    Bytes: {segment['start_byte']}-{segment['end_byte']}\")\n",
    "    \n",
    "    # Show the actual code content (truncated if too long)\n",
    "    code = segment.get('code', '')\n",
    "    if code:\n",
    "        lines = code.splitlines()\n",
    "        if len(lines) <= 8:\n",
    "            print(f\"    Code:\")\n",
    "            for i, line in enumerate(lines):\n",
    "                print(f\"      {segment.get('start_line', 0) + i + 1:4d}: {line}\")\n",
    "        else:\n",
    "            print(f\"    Code (showing first 4 and last 4 lines):\")\n",
    "            for i, line in enumerate(lines[:4]):\n",
    "                print(f\"      {segment.get('start_line', 0) + i + 1:4d}: {line}\")\n",
    "            print(f\"      ... ({len(lines) - 8} lines omitted) ...\")\n",
    "            for i, line in enumerate(lines[-4:]):\n",
    "                actual_line = segment.get('start_line', 0) + len(lines) - 4 + i + 1\n",
    "                print(f\"      {actual_line:4d}: {line}\")\n",
    "    \n",
    "    print(f\"    File: {file_path}\")\n",
    "\n",
    "def visualize_language_distribution(language_stats: Dict):\n",
    "    \"\"\"Create a visualization of language distribution\"\"\"\n",
    "    if not language_stats:\n",
    "        print(\"No language statistics to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    languages = list(language_stats.keys())\n",
    "    counts = [stats['count'] for stats in language_stats.values()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.pie(counts, labels=languages, autopct='%1.1f%%')\n",
    "    plt.title('File Distribution by Language')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(languages, counts)\n",
    "    plt.title('File Count by Language')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_segment_analysis(segments_by_language: Dict):\n",
    "    \"\"\"Create a visualization of segment analysis\"\"\"\n",
    "    if not segments_by_language:\n",
    "        print(\"No segment data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    languages = list(segments_by_language.keys())\n",
    "    segment_counts = [len(segments) for segments in segments_by_language.values()]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(languages, segment_counts)\n",
    "    plt.title('Segments Found by Language')\n",
    "    plt.ylabel('Number of Segments')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Segment types distribution\n",
    "    all_segments = []\n",
    "    for segments in segments_by_language.values():\n",
    "        all_segments.extend(segments)\n",
    "    \n",
    "    if all_segments:\n",
    "        segment_types = {}\n",
    "        for segment in all_segments:\n",
    "            seg_type = segment.get('node_type', 'unknown')\n",
    "            segment_types[seg_type] = segment_types.get(seg_type, 0) + 1\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie(segment_types.values(), labels=segment_types.keys(), autopct='%1.1f%%')\n",
    "        plt.title('Segment Types Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e004ca4",
   "metadata": {},
   "source": [
    "## 1. Input Layer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e17ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input_layer(repo_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Test the complete input layer pipeline\"\"\"\n",
    "    print(\" Running Input Layer...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize components\n",
    "    scanner = RepositoryScanner(repo_path)\n",
    "    classifier = FileClassifier()\n",
    "    queue = Preprocessor()\n",
    "    \n",
    "    # Step 1: Scan files\n",
    "    print(\" Step 1: Scanning repository...\")\n",
    "    files = scanner.scan()\n",
    "    print(f\" Scanned {len(files)} files.\")\n",
    "    \n",
    "    # Step 2: Classify and enqueue\n",
    "    print(\"\\n  Step 2: Classifying files...\")\n",
    "    language_stats = {}\n",
    "    total_loc = 0\n",
    "    \n",
    "    for path, content in files:\n",
    "        metadata = classifier.classify_file(path, content)\n",
    "        queue.enqueue(metadata)\n",
    "        \n",
    "        # Track statistics\n",
    "        if metadata.language not in language_stats:\n",
    "            language_stats[metadata.language] = {\n",
    "                'count': 0,\n",
    "                'total_loc': 0,\n",
    "                'files': []\n",
    "            }\n",
    "        \n",
    "        language_stats[metadata.language]['count'] += 1\n",
    "        language_stats[metadata.language]['total_loc'] += metadata.loc\n",
    "        language_stats[metadata.language]['files'].append(str(path))\n",
    "        total_loc += metadata.loc\n",
    "    \n",
    "    # Step 3: Print summary\n",
    "    print(f\"\\n Preprocessing Queue Summary ({len(queue.peek_all())} files):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for meta in queue.peek_all():\n",
    "        print(f\" {meta.path} | Language: {meta.language} | LOC: {meta.loc} | \"\n",
    "              f\"Complexity: {meta.complexity_score:.2f} | Priority: {meta.priority} | Difficulty: {meta.difficulty}\")\n",
    "    \n",
    "    # Print language statistics\n",
    "    print(f\"\\n Language Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    for lang, stats in language_stats.items():\n",
    "        print(f\"{lang}: {stats['count']} files, {stats['total_loc']} LOC\")\n",
    "    \n",
    "    print(f\"\\n Total: {len(files)} files, {total_loc} LOC\")\n",
    "    \n",
    "    # Visualize results\n",
    "    if language_stats:\n",
    "        visualize_language_distribution(language_stats)\n",
    "    \n",
    "    return {\n",
    "        'files': files,\n",
    "        'queue': queue,\n",
    "        'language_stats': language_stats,\n",
    "        'total_files': len(files),\n",
    "        'total_loc': total_loc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b550f2",
   "metadata": {},
   "source": [
    "## 2. Analysis Layer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a30386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analysis_layer(repo_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Test the complete analysis layer pipeline\"\"\"\n",
    "    print(\" Running Analysis Layer...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize components\n",
    "    scanner = RepositoryScanner(repo_path)\n",
    "    classifier = FileClassifier()\n",
    "    segmentation_engine = SegmentationEngine()\n",
    "    dependency_analyzer = DependencyAnalyzer()\n",
    "    \n",
    "    # Scan repository\n",
    "    files_with_chunks = scanner.scan_with_chunks()\n",
    "    \n",
    "    results = {\n",
    "        \"total_files\": 0,\n",
    "        \"supported_languages\": 0,\n",
    "        \"segments_found\": 0,\n",
    "        \"language_stats\": {},\n",
    "        \"file_details\": [],\n",
    "        \"segments_by_language\": {}\n",
    "    }\n",
    "    \n",
    "    for file_path, chunks in files_with_chunks:\n",
    "        content = \"\\n\".join(chunks)\n",
    "        metadata = classifier.classify_file(file_path, content)\n",
    "        metadata.content = content\n",
    "        \n",
    "        results[\"total_files\"] += 1\n",
    "        \n",
    "        print(f\"\\n File: {file_path}\")\n",
    "        print(f\"   Language: {metadata.language}\")\n",
    "        print(f\"   Size: {metadata.size} bytes, Lines: {metadata.loc}\")\n",
    "        \n",
    "        # Track language statistics\n",
    "        if metadata.language not in results[\"language_stats\"]:\n",
    "            results[\"language_stats\"][metadata.language] = {\n",
    "                \"count\": 0,\n",
    "                \"total_segments\": 0,\n",
    "                \"files\": []\n",
    "            }\n",
    "        \n",
    "        results[\"language_stats\"][metadata.language][\"count\"] += 1\n",
    "        \n",
    "        # Test segmentation\n",
    "        segments = []\n",
    "        error = None\n",
    "        try:\n",
    "            segments = segmentation_engine.segment_code(metadata)\n",
    "            \n",
    "            if segments:\n",
    "                results[\"supported_languages\"] += 1\n",
    "                results[\"segments_found\"] += len(segments)\n",
    "                results[\"language_stats\"][metadata.language][\"total_segments\"] += len(segments)\n",
    "                \n",
    "                # Store segments by language for visualization\n",
    "                if metadata.language not in results[\"segments_by_language\"]:\n",
    "                    results[\"segments_by_language\"][metadata.language] = []\n",
    "                results[\"segments_by_language\"][metadata.language].extend(segments)\n",
    "                \n",
    "                print(f\"    Segments found: {len(segments)}\")\n",
    "                \n",
    "                # Show detailed segment information\n",
    "                for i, segment in enumerate(segments):\n",
    "                    print_segment_details(segment, i, str(file_path))\n",
    "                \n",
    "                # Test context extraction for first segment\n",
    "                if segments:\n",
    "                    first_segment = segments[0]\n",
    "                    context_result = extract_context(metadata, first_segment[\"start_byte\"])\n",
    "                    context_lines = len(context_result[\"context_code\"].splitlines())\n",
    "                    print(f\"    Context lines for first segment: {context_lines}\")\n",
    "                    \n",
    "                    # Show context content if not too long\n",
    "                    context_code = context_result[\"context_code\"]\n",
    "                    if context_code:\n",
    "                        context_lines_list = context_code.splitlines()\n",
    "                        if len(context_lines_list) <= 5:\n",
    "                            print(f\"   Context content:\")\n",
    "                            for line in context_lines_list:\n",
    "                                print(f\"     {line}\")\n",
    "                        else:\n",
    "                            print(f\"   Context content (first 3 lines):\")\n",
    "                            for line in context_lines_list[:3]:\n",
    "                                print(f\"     {line}\")\n",
    "                            print(f\"     ... ({len(context_lines_list) - 3} more lines)\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"     No segments found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            print(f\"    Error during segmentation: {e}\")\n",
    "        \n",
    "        # Store file details\n",
    "        file_detail = {\n",
    "            \"path\": str(file_path),\n",
    "            \"language\": metadata.language,\n",
    "            \"size\": metadata.size,\n",
    "            \"loc\": metadata.loc,\n",
    "            \"segments_count\": len(segments),\n",
    "            \"error\": error\n",
    "        }\n",
    "        results[\"file_details\"].append(file_detail)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total files processed: {results['total_files']}\")\n",
    "    print(f\"Files with supported languages: {results['supported_languages']}\")\n",
    "    print(f\"Total segments found: {results['segments_found']}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    if results[\"segments_by_language\"]:\n",
    "        visualize_segment_analysis(results[\"segments_by_language\"])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a5943",
   "metadata": {},
   "source": [
    "## 3. Complete Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87fca6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_complete_pipeline(repo_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Test the complete pipeline (input + analysis layers)\"\"\"\n",
    "    print(\" Running Complete Pipeline Test...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test Input Layer\n",
    "    print(\"\\n PHASE 1: Input Layer Testing\")\n",
    "    input_results = test_input_layer(repo_path)\n",
    "    \n",
    "    # Test Analysis Layer\n",
    "    print(\"\\n PHASE 2: Analysis Layer Testing\")\n",
    "    analysis_results = test_analysis_layer(repo_path)\n",
    "    \n",
    "    # Combine results\n",
    "    combined_results = {\n",
    "        'input_layer': input_results,\n",
    "        'analysis_layer': analysis_results,\n",
    "        'summary': {\n",
    "            'total_files': input_results['total_files'],\n",
    "            'total_loc': input_results['total_loc'],\n",
    "            'supported_languages': analysis_results['supported_languages'],\n",
    "            'total_segments': analysis_results['segments_found'],\n",
    "            'languages_found': list(input_results['language_stats'].keys())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" PIPELINE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\" Total files processed: {combined_results['summary']['total_files']}\")\n",
    "    print(f\" Total lines of code: {combined_results['summary']['total_loc']}\")\n",
    "    print(f\"🔧 Supported languages: {combined_results['summary']['supported_languages']}\")\n",
    "    print(f\" Total segments found: {combined_results['summary']['total_segments']}\")\n",
    "    print(f\" Languages found: {', '.join(combined_results['summary']['languages_found'])}\")\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61aea1",
   "metadata": {},
   "source": [
    "## 4. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the current repository (aggie_nexus_mvp)\n",
    "print(\" Testing the current repository...\")\n",
    "test_complete_pipeline(\"./aggie_nexus_mvp\")\n",
    "\n",
    "# Example: Test a GitHub repository\n",
    "# test_complete_pipeline(\"https://github.com/username/repo-name\")\n",
    "\n",
    "# Example: Test a local repository\n",
    "# test_complete_pipeline(\"./your-local-repo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
